import pandas as pd
import pymongo
import akshare as ak
import jieba
from datetime import datetime
from clickhouse_driver import Client
from nlp_stocks import load_resources, BLACKLIST
from llm_judge import analyze_news_impact

# Connect to Database
mongo_client = pymongo.MongoClient("...")
news_collection = mongo_client["stock_data"]["news_cailianshe"]
# ClickHouse Connection
ch_client = Client(host='...', user='...', password='...', database='stock_data', settings={'use_numpy': True})

def get_market_caps(stock_codes):
    """
    æ‰¹é‡è·å–è‚¡ç¥¨çš„æœ€æ–°å¸‚å€¼ (RAG çš„æ ¸å¿ƒæ•°æ®æº)
    """
    print("æ­£åœ¨æŸ¥è¯¢æœ€æ–°å¸‚å€¼æ•°æ®...")
    try:
        # è·å–å…¨å¸‚åœºå®æ—¶è¡Œæƒ…
        df = ak.stock_zh_a_spot_em()
        # ç­›é€‰å‡ºéœ€è¦çš„è‚¡ç¥¨
        df = df[df['ä»£ç '].isin(stock_codes)]
        
        # map: code -> market_cap (äº¿å…ƒ)
        # AKShare è¿”å›çš„ 'æ€»å¸‚å€¼' å•ä½æ˜¯å…ƒï¼Œæˆ‘ä»¬éœ€è¦è½¬æˆäº¿å…ƒæ–¹ä¾¿ AI ç†è§£
        market_cap_map = {}
        for _, row in df.iterrows():
            code = row['ä»£ç ']
            # æ€»å¸‚å€¼å¯èƒ½å¾ˆå¤§ï¼Œè½¬ä¸ºâ€œäº¿â€ä¸ºå•ä½ï¼Œä¿ç•™2ä½å°æ•°
            mkt_cap_yi = round(row['æ€»å¸‚å€¼'] / 100000000, 2)
            market_cap_map[code] = mkt_cap_yi
            
        return market_cap_map
    except Exception as e:
        print(f"è·å–å¸‚å€¼å¤±è´¥: {e}")
        return {}
    
def save_results(results):
    """
    å°†åˆ†æç»“æœæ‰¹é‡å†™å…¥ ClickHouse
    """
    if not results: return

    print(f"æ­£åœ¨å°† {len(results)} æ¡å› å­æ•°æ®å­˜å…¥ ClickHouse...")
    
    data_to_insert = []
    today = datetime.now().date()
    
    for res in results:
        # æ„é€ ä¸€è¡Œæ•°æ®
        row = {
            'ts_code': res['code'],
            'trade_date': today, # å®é™…åº”å–æ–°é—»å‘å¸ƒæ—¶é—´å¯¹åº”çš„äº¤æ˜“æ—¥ï¼Œè¿™é‡Œå…ˆç®€åŒ–ä¸ºä»Šå¤©
            'publish_time': res['publish_time'],
            'news_title': res['title'],
            'score': res['score'],
            'magnitude': res.get('magnitude', 0.0),
            'certainty': res.get('certainty', 0.0),
            'reason': res['reason']
        }
        data_to_insert.append(row)
    
    # è½¬ DataFrame
    df = pd.DataFrame(data_to_insert)
    
    try:
        ch_client.insert_dataframe(
            'INSERT INTO stock_news_sentiment (ts_code, trade_date, publish_time, news_title, score, magnitude, certainty, reason) VALUES',
            df
        )
        print("å› å­å…¥åº“æˆåŠŸï¼")
    except Exception as e:
        print(f"å…¥åº“å¤±è´¥: {e}")


def run_ai_strategy():
    maps = load_resources()
    if not maps: return
    alias_map, name_map = maps

    print("\nğŸ“° 1. æ‰«ææœ€è¿‘æ–°é—»...")
    # æ‰«ææœ€è¿‘ 20 æ¡ç”¨äºæµ‹è¯•
    recent_news = list(news_collection.find().sort("crawled_at", -1).limit(20))
    stock_news_map = {} 

    # News time
    for news in recent_news:
        content = news.get('content') or news.get('å†…å®¹') or ''
        title = news.get('title') or news.get('æ ‡é¢˜') or 'å¿«è®¯'
    
        pub_time_str = news.get('publish_time') or news.get('time')
    
        try:
            if pub_time_str:
                if len(pub_time_str) > 10:
                    pub_time = datetime.strptime(str(pub_time_str), "%Y-%m-%d %H:%M:%S")
                else:
                    pub_time = datetime.now() # åªæœ‰æ—¶åˆ†ç§’çš„æƒ…å†µæš‚ç•¥
            else:
                pub_time = datetime.now()
        except:
            pub_time = datetime.now()

        full_text = f"{title} {content}"
        words = jieba.lcut(full_text)
        seen_in_this_news = set()
        
        for w in words:
            if w in BLACKLIST or len(w) < 2: continue
            if w in alias_map:
                code = alias_map[w]
                if code not in seen_in_this_news:
                    if code not in stock_news_map:
                        stock_news_map[code] = []
                    # tuple (å†…å®¹, æ ‡é¢˜, æ—¶é—´)
                    stock_news_map[code].append((full_text[:500], title, pub_time))
                    seen_in_this_news.add(code)

    if not stock_news_map:
        print("æ²¡æœ‰æ£€æµ‹åˆ°ç›¸å…³è‚¡ç¥¨ã€‚")
        return

    # RAG
    target_codes = list(stock_news_map.keys())
    market_cap_map = get_market_caps(target_codes)
    print(f"æ‰¾åˆ° {len(stock_news_map)} åªè‚¡ç¥¨, å¼€å§‹ AI è¯„åˆ†...")
    
    results = []

    # LLM
    for code, items in stock_news_map.items():
        name = name_map.get(code, "æœªçŸ¥")
        market_cap = market_cap_map.get(code, "æœªçŸ¥")
        
        # åªåˆ†ææœ€æ–°çš„ä¸€æ¡
        latest_item = items[0] 
        news_content = latest_item[0]
        news_title = latest_item[1]
        pub_time = latest_item[2]
        
        # è°ƒç”¨ AI
        ai_result = analyze_news_impact(name, code, market_cap, news_content)
        
        results.append({
            'code': code,
            'name': name,
            'publish_time': pub_time,
            'title': news_title,
            'score': ai_result['final_score'],
            'magnitude': ai_result.get('magnitude', 0),
            'certainty': ai_result.get('certainty', 0),
            'reason': ai_result['reason']
        })

    save_results(results)

if __name__ == "__main__":
    run_ai_strategy()
